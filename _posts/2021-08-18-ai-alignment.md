---
layout: post
title:  "What I know about getting started with AI-alignment"
date:   2021-08-18 22:55:31 -0700
categories: AI alignment EA effective-altruism
author: Roy Rinberg
---

<nav class="toc" style = "margin-bottom: 0;" markdown="1">
**Table of Contents**

* TOC
{:toc}
</nav>


While I'm not personally working on AI-alignment, I think it's likely pretty important, and want to help increase the knowledge-base around it and access-to-information about it. This post is a short resource list based on personal readings and conversations I've had with friends interested in AI alignment.

## At a high-level, what is AI-alignment:

One day we will likely transfer control of important applications. If you can imagine that these AI could be  as intelligent as human beings, but lack the cultural, ethical, moral, or biological constraints that other humans have, it's fairly easy to imagine that these AI's goals may be drastically different from our own. AI-alignment is the challenge of trying to align the goals of AI agents with the goals of humans. In particular, this challenge focuses on doing so while we still have a moderate understanding of how they work, and good control over what they work on. 

## A quick example of an AI-alignment challenge:

The most common introduction to problems in AI alignment is the [paperclip maximizer](https://www.lesswrong.com/tag/paperclip-maximizer). This relates to [Instrumental Convergence](https://en.wikipedia.org/wiki/Instrumental_convergence), where an intelligent actor is given a straightforward goal, but may act in surprising or harmful ways. 

> Instrumental convergence posits that an intelligent agent with unbounded but apparently harmless goals can act in surprisingly harmful ways. For example, a computer with the sole, unconstrained goal of solving an incredibly difficult mathematics problem like the Riemann hypothesis could attempt to turn the entire Earth into one giant computer in an effort to increase its computational power so that it can succeed in its calculations - [Wikipedia](https://en.wikipedia.org/wiki/Instrumental_convergence)

Hopefully this simple example of a challenge in AI alignment sparks your interest a bit. From here on, all I can do is point you to a bunch of other resources on AI-alignment which do a much better job discussing it.

## Educational Resources

* [Robert Miles' Youtube Channel](https://www.youtube.com/c/RobertMilesAI/playlists)
* [AI alignment, why it's hard and where to start (by MIRI)](https://intelligence.org/2016/12/28/ai-alignment-why-its-hard-and-where-to-start/), and the corresponding [youtube link](https://www.youtube.com/watch?v=EUjc1WuyPT8).
* [AI-Alignment Forum](https://www.alignmentforum.org/)
* [AI-Alignment Review google doc](https://docs.google.com/document/d/1Fng1J_QPb7GEeLBMmWWfZOguw7yUTZot0egrCbKpVwk/edit#)
* [Effective Altruism AGI Educational Fellowship](https://www.eacambridge.org/agi-safety-fundamentals). (AGI is Artificial General Intelligence).

* [Interview with Dr. Paul Christiano about AI alignment work at OpenAI](https://80000hours.org/podcast/episodes/paul-christiano-ai-alignment-solutions/) (circa 2018). Interview on 80,000 hours podcast.
* [Dr. Paul Christiano's website](https://sideways-view.com/)
* [Concrete Problems in AI Safety](https://arxiv.org/pdf/1606.06565v1.pdf). (This is an academic paper circa 2016, so it may not be a good place to start, and is likely out-of-date).

## Some organizations working on this

* [Anthropic](https://www.anthropic.com/)
* [OpenAI](https://openai.com/)
* [Machine Intelligence Research Institute (MIRI)](https://intelligence.org/)
* [Center for Human-Compatible Artificial Intelligence (CHAI)](https://humancompatible.ai/)
* [AI Objectives](https://ai.objectives.institute/people)
